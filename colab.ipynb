{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "colab.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN7X42Oo+ZCK7LKT6L21mzr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TomFrederik/learning_training_dynamics/blob/main/colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ewTXBKuxZuzr",
        "outputId": "9c0ae97e-b5dc-46f5-fb48-d9c376a93658"
      },
      "source": [
        "#!pip install torchdiffeq # only run on first execution\n",
        "#!git clone https://github.com/TomFrederik/learning_training_dynamics.git\n",
        "%cd learning_training_dynamics\n",
        "#!git pull"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/learning_training_dynamics\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YJU9FSShZUad"
      },
      "source": [
        "import torchdiffeq as teq\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import os\n",
        "\n",
        "import datasets"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0N7o3u4luzd4"
      },
      "source": [
        "class MLP(nn.Module):\n",
        "\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        self.net = nn.Sequential(nn.Linear(input_dim, 200), nn.Tanh(), nn.Linear(200, 200), nn.Tanh(), nn.Linear(200, 200), nn.Tanh(), nn.Linear(200, output_dim))\n",
        "        self.device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
        "    \n",
        "    def forward(self, t, x):\n",
        "        x = torch.cat([torch.tensor([t], device=self.device), x], dim=-1)\n",
        "        return self.net(x)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYqON_I8vxFX"
      },
      "source": [
        "def rms_norm(tensor):\n",
        "    print(tensor)\n",
        "    return tensor.pow(2).mean().sqrt()\n",
        "\n",
        "def make_norm(state):\n",
        "    state_size = state.numel()\n",
        "    def norm(aug_state):\n",
        "        print(f'aug state = {aug_state}')\n",
        "        y = aug_state[1:1 + state_size]\n",
        "        adj_y = aug_state[1 + state_size:1 + 2 * state_size] \n",
        "        print(f'y = {y}')\n",
        "        print('rms_norm(y) = ')\n",
        "        print(rms_norm(y))\n",
        "        print('rms_norm(adj_y) = ')\n",
        "        print(rms_norm(adj_y))\n",
        "        return max(rms_norm(y), rms_norm(adj_y))\n",
        "    return norm\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l1wTtVhHZbkL"
      },
      "source": [
        "# some hyperparams\n",
        "hidden_dim = 100\n",
        "train_steps = 1000\n",
        "lr = 1e-3\n",
        "\n",
        "# set device\n",
        "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# set up data\n",
        "data_dir = './data/mini_mnist'\n",
        "train_dataset = datasets.MiniMNISTParams(data_dir)\n",
        "#test_dataset = datasets.MiniMNISTParams(data_dir, train=False)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=1)\n",
        "#test_loader = DataLoader(test_dataset, batch_size=1)\n",
        "\n",
        "# \n",
        "input_dim = train_dataset[0][1].shape[1]\n",
        "time_stamps = torch.arange(0, train_dataset[0][1].shape[0], 1, dtype=float, device=device)\n",
        "\n",
        "\n",
        "# set up model\n",
        "model_kwargs = {'input_dim': input_dim+1, # +1 for time  \n",
        "                'hidden_dim': hidden_dim, # is cur 100, should change to [20,20]\n",
        "                'output_dim':input_dim\n",
        "                }\n",
        "model = MLP(**model_kwargs).to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "loss_fct = nn.MSELoss()"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XfpRnYwaZfrz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "134157b1-030f-4212-d609-6299d5db3f27"
      },
      "source": [
        "# training\n",
        "step = 0\n",
        "epoch = 0\n",
        "while step < train_steps:\n",
        "\n",
        "    epoch += 1\n",
        "    print(f'\\nStarting epoch {epoch}')\n",
        "    model.train()\n",
        "    for y_0, y in iter(train_loader):\n",
        "        step += 1\n",
        "        \n",
        "        y_0 = y_0.squeeze().to(device)\n",
        "        y = y.squeeze().to(device)\n",
        "        \n",
        "        # train step\n",
        "        optimizer.zero_grad()\n",
        "        pred = teq.odeint_adjoint(model, y_0, time_stamps, adjoint_options=dict(norm='seminorm'))\n",
        "        loss = loss_fct(pred, y.squeeze())\n",
        "        print(f'Loss at step {step} is {loss.item():1.3f}')\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # eval step\n",
        "    #model.eval()"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Starting epoch 1\n",
            "Loss at step 1 is 0.883\n",
            "Loss at step 2 is 4.949\n",
            "Loss at step 3 is 3.521\n",
            "Loss at step 4 is 2.485\n",
            "Loss at step 5 is 1.925\n",
            "Loss at step 6 is 1.539\n",
            "Loss at step 7 is 1.142\n",
            "Loss at step 8 is 0.869\n",
            "Loss at step 9 is 0.876\n",
            "Loss at step 10 is 0.808\n",
            "Loss at step 11 is 0.755\n",
            "Loss at step 12 is 0.679\n",
            "Loss at step 13 is 0.648\n",
            "Loss at step 14 is 0.640\n",
            "Loss at step 15 is 0.607\n",
            "\n",
            "Starting epoch 2\n",
            "Loss at step 16 is 0.558\n",
            "Loss at step 17 is 0.590\n",
            "Loss at step 18 is 0.536\n",
            "Loss at step 19 is 0.492\n",
            "Loss at step 20 is 0.466\n",
            "Loss at step 21 is 0.427\n",
            "Loss at step 22 is 0.410\n",
            "Loss at step 23 is 0.399\n",
            "Loss at step 24 is 0.379\n",
            "Loss at step 25 is 0.347\n",
            "Loss at step 26 is 0.331\n",
            "Loss at step 27 is 0.325\n",
            "Loss at step 28 is 0.314\n",
            "Loss at step 29 is 0.302\n",
            "Loss at step 30 is 0.287\n",
            "\n",
            "Starting epoch 3\n",
            "Loss at step 31 is 0.273\n",
            "Loss at step 32 is 0.271\n",
            "Loss at step 33 is 0.263\n",
            "Loss at step 34 is 0.271\n",
            "Loss at step 35 is 0.260\n",
            "Loss at step 36 is 0.247\n",
            "Loss at step 37 is 0.234\n",
            "Loss at step 38 is 0.224\n",
            "Loss at step 39 is 0.219\n",
            "Loss at step 40 is 0.212\n",
            "Loss at step 41 is 0.200\n",
            "Loss at step 42 is 0.192\n",
            "Loss at step 43 is 0.187\n",
            "Loss at step 44 is 0.181\n",
            "Loss at step 45 is 0.172\n",
            "\n",
            "Starting epoch 4\n",
            "Loss at step 46 is 0.167\n",
            "Loss at step 47 is 0.162\n",
            "Loss at step 48 is 0.152\n",
            "Loss at step 49 is 0.146\n",
            "Loss at step 50 is 0.142\n",
            "Loss at step 51 is 0.133\n",
            "Loss at step 52 is 0.127\n",
            "Loss at step 53 is 0.121\n",
            "Loss at step 54 is 0.116\n",
            "Loss at step 55 is 0.111\n",
            "Loss at step 56 is 0.104\n",
            "Loss at step 57 is 0.098\n",
            "Loss at step 58 is 0.093\n",
            "Loss at step 59 is 0.088\n",
            "Loss at step 60 is 0.083\n",
            "\n",
            "Starting epoch 5\n",
            "Loss at step 61 is 0.079\n",
            "Loss at step 62 is 0.073\n",
            "Loss at step 63 is 0.070\n",
            "Loss at step 64 is 0.066\n",
            "Loss at step 65 is 0.063\n",
            "Loss at step 66 is 0.056\n",
            "Loss at step 67 is 0.054\n",
            "Loss at step 68 is 0.049\n",
            "Loss at step 69 is 0.046\n",
            "Loss at step 70 is 0.042\n",
            "Loss at step 71 is 0.039\n",
            "Loss at step 72 is 0.036\n",
            "Loss at step 73 is 0.033\n",
            "Loss at step 74 is 0.031\n",
            "Loss at step 75 is 0.029\n",
            "\n",
            "Starting epoch 6\n",
            "Loss at step 76 is 0.027\n",
            "Loss at step 77 is 0.026\n",
            "Loss at step 78 is 0.024\n",
            "Loss at step 79 is 0.022\n",
            "Loss at step 80 is 0.020\n",
            "Loss at step 81 is 0.019\n",
            "Loss at step 82 is 0.017\n",
            "Loss at step 83 is 0.016\n",
            "Loss at step 84 is 0.014\n",
            "Loss at step 85 is 0.014\n",
            "Loss at step 86 is 0.013\n",
            "Loss at step 87 is 0.011\n",
            "Loss at step 88 is 0.011\n",
            "Loss at step 89 is 0.009\n",
            "Loss at step 90 is 0.009\n",
            "\n",
            "Starting epoch 7\n",
            "Loss at step 91 is 0.009\n",
            "Loss at step 92 is 0.009\n",
            "Loss at step 93 is 0.011\n",
            "Loss at step 94 is 0.008\n",
            "Loss at step 95 is 0.010\n",
            "Loss at step 96 is 0.006\n",
            "Loss at step 97 is 0.007\n",
            "Loss at step 98 is 0.006\n",
            "Loss at step 99 is 0.006\n",
            "Loss at step 100 is 0.006\n",
            "Loss at step 101 is 0.005\n",
            "Loss at step 102 is 0.005\n",
            "Loss at step 103 is 0.005\n",
            "Loss at step 104 is 0.005\n",
            "Loss at step 105 is 0.004\n",
            "\n",
            "Starting epoch 8\n",
            "Loss at step 106 is 0.005\n",
            "Loss at step 107 is 0.007\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-e0edf6b1e9cc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Loss at step {step} is {loss.item():1.3f}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    130\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sx5ojiGlOumY"
      },
      "source": [
        "# save model\n",
        "torch.save(model.state_dict(), './trained_model.pt')"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QiWD41lp_Avk"
      },
      "source": [
        "# base model class\n",
        "class BaseMLP(nn.Module):\n",
        "\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        layers = [nn.Linear(input_dim, hidden_dim[0]), nn.ReLU()]\n",
        "        for i in range(1,len(hidden_dim)):\n",
        "            layers.append(nn.Linear(hidden_dim[i-1], hidden_dim[i]))\n",
        "            layers.append(nn.ReLU())\n",
        "        layers.append(nn.Linear(hidden_dim[-1], output_dim))\n",
        "        \n",
        "        self.net = nn.Sequential(*layers)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.net(x)"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7TJ2xiSnAJpN"
      },
      "source": [
        "def get_params(model):\n",
        "    '''\n",
        "    Returns the parameters of a model as a single vector\n",
        "    Args:\n",
        "        model - instance of torch.nn.Module\n",
        "    Returns:\n",
        "        params - list of shape [n_params]\n",
        "    '''\n",
        "    params = []\n",
        "    for param in model.parameters():\n",
        "        params.extend(param.flatten().detach().tolist())\n",
        "    return params"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kaokbEbTFGNh"
      },
      "source": [
        "def params2weights(param_vector, param_ranges):\n",
        "    '''\n",
        "    Maps a vector of parameters to a list of tensors capturing the model weights and biases\n",
        "    Args:\n",
        "        param_vector - torch tensor of shape [n_params]\n",
        "        param_ranges - list containing number of parameters for each weight matrix or bias vector\n",
        "    Returns:\n",
        "        weights - list of torch tensors containing weights and biases: [w1, b1, w2, b2, ...]\n",
        "    '''\n",
        "    return list(torch.split(param_vector, param_ranges))\n"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lWajsUmldNGj",
        "outputId": "96791734-377c-4c41-af92-fe8cf3f21846"
      },
      "source": [
        "# initialize random model \n",
        "random_model = BaseMLP(input_dim=28*28, hidden_dim=[20,20], output_dim=1).to(device)\n",
        "\n",
        "# load data\n",
        "data_dir = './data/mini_mnist'\n",
        "\n",
        "train_dataset = datasets.MiniMNIST(data_dir, flatten=True)\n",
        "train_loader = DataLoader(train_dataset, shuffle=False, batch_size=len(train_dataset))\n",
        "\n",
        "test_dataset = datasets.MiniMNIST(data_dir, flatten=True, train=False)\n",
        "test_loader = DataLoader(test_dataset, shuffle=False, batch_size=len(test_dataset))\n",
        "\n",
        "# eval func\n",
        "loss_fct = nn.BCEWithLogitsLoss()\n",
        "\n",
        "# evluate model on train data\n",
        "random_model.eval()\n",
        "train_data, train_labels = next(iter(train_loader))\n",
        "train_data = train_data.to(device)\n",
        "train_labels = train_labels.to(device)\n",
        "train_logits = random_model(train_data).squeeze()\n",
        "train_loss = loss_fct(train_logits, train_labels).item()\n",
        "train_acc = (torch.round(torch.sigmoid(train_logits)) == train_labels).sum() / len(train_labels)\n",
        "print(f'Train loss at init is {train_loss:1.5f}')\n",
        "print(f'Train accuracy at init is {train_acc:1.5f}')\n",
        "\n",
        "\n",
        "# evluate model on test data\n",
        "random_model.eval()\n",
        "test_data, test_labels = next(iter(test_loader))\n",
        "test_data = test_data.to(device)\n",
        "test_labels = test_labels.to(device)\n",
        "test_logits = random_model(test_data).squeeze()\n",
        "test_loss = loss_fct(test_logits, test_labels).item()\n",
        "test_acc = (torch.round(torch.sigmoid(test_logits)) == test_labels).sum() / len(test_labels)\n",
        "print(f'\\nTest loss at init is {test_loss:1.5f}')\n",
        "print(f'Test accuracy at init is {test_acc:1.5f}')"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train loss at init is 2.30721\n",
            "Train accuracy at init is 0.50000\n",
            "\n",
            "Test loss at init is 2.04358\n",
            "Test accuracy at init is 0.52500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JV0TWEWXqnPh",
        "outputId": "5e1f11d9-4085-43c5-8773-9361c19e98a1"
      },
      "source": [
        "\n",
        "\n",
        "####\n",
        "# Evolve using learned dynamics\n",
        "####\n",
        "\n",
        "\n",
        "# get parameters of random model\n",
        "params = torch.tensor(get_params(random_model), device=device)\n",
        "\n",
        "# use learned dynamics model to evolve the parameters\n",
        "evolved_params = teq.odeint_adjoint(model, params, time_stamps, adjoint_options={'norm': 'seminorm'})\n",
        "params_at_25 = evolved_params[-1]\n",
        "params_at_8 = evolved_params[25]\n",
        "\n",
        "# convert param vector to fit weight dimensions of base model\n",
        "param_ranges = []\n",
        "param_shapes = []\n",
        "for param in random_model.parameters():\n",
        "    param_ranges.append(torch.prod(torch.tensor(param.shape)).item())\n",
        "    param_shapes.append(param.shape)\n",
        "\n",
        "sliced_params = params2weights(params_at_8, param_ranges)\n",
        "# reshape them into correct shape\n",
        "for i, shape in enumerate(param_shapes):\n",
        "    sliced_params[i] = sliced_params[i].reshape(shape)\n",
        "\n",
        "# init a new model\n",
        "evolved_model = BaseMLP(input_dim=28*28, hidden_dim=[20,20], output_dim=1).to(device)\n",
        "\n",
        "# set base model params to evolved params\n",
        "for old_param, new_param in zip(evolved_model.parameters(), sliced_params):\n",
        "   old_param.data = new_param\n",
        "\n",
        "# eval evolved model\n",
        "evolved_model.eval()\n",
        "test_data, test_labels = next(iter(test_loader))\n",
        "test_data = test_data.to(device)\n",
        "test_labels = test_labels.to(device)\n",
        "test_logits = evolved_model(test_data).squeeze()\n",
        "test_loss = loss_fct(test_logits, test_labels).item()\n",
        "test_acc = (torch.round(torch.sigmoid(test_logits)) == test_labels).sum() / len(test_labels)\n",
        "print(f'\\nTest loss after evolving is {test_loss:1.5f}')\n",
        "print(f'Test accuracy after evolving is {test_acc:1.5f}')\n",
        "print(f'Test logits after evolving {test_logits}')\n",
        "print(f'Test probs after evolving {torch.sigmoid(test_logits)}')\n",
        "\n",
        "####\n",
        "# train in normal way\n",
        "####\n",
        "\n",
        "def train_model(model, opt_class, opt_kwargs, loss_fct, train_loader, test_loader, train_steps):\n",
        "\n",
        "    # init param trajectory\n",
        "    params = []\n",
        "    params.append(get_params(model))\n",
        "    \n",
        "    # set up optimizer\n",
        "    optimizer = opt_class(model.parameters(), **opt_kwargs)\n",
        "\n",
        "    # initial random logits\n",
        "    #train_logits = []\n",
        "    #data, _ = next(iter(train_loader))\n",
        "    #train_logits.append(model(data).squeeze().tolist())\n",
        "\n",
        "    for step in range(train_steps):\n",
        "        \n",
        "        # one training step\n",
        "        model.train()\n",
        "        data, labels = next(iter(train_loader))\n",
        "        data = data.to(device)\n",
        "        labels = labels.to(device)\n",
        "        logits = model(data).squeeze()\n",
        "        loss = loss_fct(logits, labels)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # store new params\n",
        "        params.append(get_params(model))\n",
        "\n",
        "        # store output of network at this step\n",
        "        #train_logits.append(logits.tolist())\n",
        "\n",
        "        # eval on test set\n",
        "        model.eval()\n",
        "        test_data, test_labels = next(iter(test_loader))\n",
        "        test_data = test_data.to(device)\n",
        "        test_labels = test_labels.to(device)\n",
        "        test_logits = model(test_data).squeeze()\n",
        "        test_loss = loss_fct(test_logits, test_labels)\n",
        "        test_acc = torch.sum(torch.round(torch.sigmoid(test_logits)) == test_labels) / len(test_labels)\n",
        "\n",
        "        print(f'Step {step+1}: train loss = {loss.item():1.5f}, test loss = {test_loss.item():1.5f}, test acc = {test_acc.item():1.4f}')\n",
        "                \n",
        "\n",
        "    return params\n",
        "\n",
        "# optimizer\n",
        "opt_class = torch.optim.Adam\n",
        "opt_kwargs = {'lr':lr}\n",
        "\n",
        "# loss function\n",
        "loss_fct = torch.nn.BCEWithLogitsLoss()\n",
        "\n",
        "train_steps = 25\n",
        "lr = 1e-4\n",
        "\n",
        "train_kwargs = {'opt_class':opt_class,\n",
        "                'opt_kwargs':opt_kwargs, \n",
        "                'loss_fct':loss_fct, \n",
        "                'train_loader':train_loader, \n",
        "                'test_loader':test_loader, \n",
        "                'train_steps':train_steps\n",
        "                }\n",
        "\n",
        "# train the model\n",
        "trained_params = torch.tensor(train_model(random_model, **train_kwargs), device=device).view(evolved_params.shape)\n",
        "\n",
        "# compare evolved with trained\n",
        "mse_loss = nn.MSELoss(reduction='none')\n",
        "evol_loss = mse_loss(trained_params, evolved_params).mean(dim=1)\n",
        "print(evol_loss)"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test loss after evolving is 23.34353\n",
            "Test accuracy after evolving is 0.50000\n",
            "Test logits after evolving tensor([ -91.3628,  -63.6737,  -84.7311,  -81.5237,  -55.1757,  -59.5826,\n",
            "         -59.3035, -101.9971,  -51.6361, -108.9850,  -18.5841, -115.8868,\n",
            "         -55.6021,  -53.9199,  -80.9800,   -0.4303,  -14.1597,  -15.8407,\n",
            "         -88.3558,  -38.3715,  -42.9586,  -45.3851,  -67.4790,   -7.7007,\n",
            "         -36.7669,  -39.1791,  -19.7898,  -28.4659,  -18.9605,  -58.8635,\n",
            "         -47.1996,  -38.9131,  -62.8426,  -98.1344,  -48.8066,  -52.9295,\n",
            "         -43.5422,  -39.6262,  -56.8791,  -78.8172], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>)\n",
            "Test probs after evolving tensor([0.0000e+00, 2.2225e-28, 1.5913e-37, 3.9328e-36, 1.0901e-24, 1.3292e-26,\n",
            "        1.7572e-26, 0.0000e+00, 3.7561e-23, 0.0000e+00, 8.4922e-09, 0.0000e+00,\n",
            "        7.1176e-25, 3.8270e-24, 6.7737e-36, 3.9406e-01, 7.0879e-07, 1.3197e-07,\n",
            "        4.2418e-39, 2.1650e-17, 2.2046e-19, 1.9475e-20, 4.9458e-30, 4.5230e-04,\n",
            "        1.0773e-16, 9.6544e-18, 2.5432e-09, 4.3391e-13, 5.8285e-09, 2.7285e-26,\n",
            "        3.1730e-21, 1.2596e-17, 5.1030e-28, 0.0000e+00, 6.3617e-22, 1.0304e-23,\n",
            "        1.2299e-19, 6.1740e-18, 1.9848e-25, 5.8900e-35], device='cuda:0',\n",
            "       grad_fn=<SigmoidBackward>)\n",
            "Step 1: train loss = 2.30721, test loss = 1.81879, test acc = 0.5500\n",
            "Step 2: train loss = 2.05072, test loss = 1.61266, test acc = 0.5500\n",
            "Step 3: train loss = 1.80111, test loss = 1.41349, test acc = 0.5750\n",
            "Step 4: train loss = 1.56054, test loss = 1.21866, test acc = 0.6250\n",
            "Step 5: train loss = 1.33320, test loss = 1.03719, test acc = 0.6250\n",
            "Step 6: train loss = 1.12224, test loss = 0.87051, test acc = 0.6500\n",
            "Step 7: train loss = 0.92786, test loss = 0.72240, test acc = 0.6750\n",
            "Step 8: train loss = 0.75479, test loss = 0.59084, test acc = 0.7250\n",
            "Step 9: train loss = 0.59910, test loss = 0.47285, test acc = 0.7500\n",
            "Step 10: train loss = 0.46238, test loss = 0.37456, test acc = 0.8250\n",
            "Step 11: train loss = 0.35001, test loss = 0.29430, test acc = 0.8250\n",
            "Step 12: train loss = 0.26288, test loss = 0.23282, test acc = 0.8750\n",
            "Step 13: train loss = 0.19841, test loss = 0.17958, test acc = 0.9000\n",
            "Step 14: train loss = 0.14995, test loss = 0.13702, test acc = 0.9500\n",
            "Step 15: train loss = 0.11430, test loss = 0.10380, test acc = 0.9500\n",
            "Step 16: train loss = 0.08762, test loss = 0.07808, test acc = 1.0000\n",
            "Step 17: train loss = 0.06760, test loss = 0.05786, test acc = 1.0000\n",
            "Step 18: train loss = 0.05232, test loss = 0.04285, test acc = 1.0000\n",
            "Step 19: train loss = 0.04094, test loss = 0.03159, test acc = 1.0000\n",
            "Step 20: train loss = 0.03233, test loss = 0.02386, test acc = 1.0000\n",
            "Step 21: train loss = 0.02607, test loss = 0.01845, test acc = 1.0000\n",
            "Step 22: train loss = 0.02135, test loss = 0.01460, test acc = 1.0000\n",
            "Step 23: train loss = 0.01759, test loss = 0.01188, test acc = 1.0000\n",
            "Step 24: train loss = 0.01477, test loss = 0.00996, test acc = 1.0000\n",
            "Step 25: train loss = 0.01263, test loss = 0.00859, test acc = 1.0000\n",
            "tensor([0.0000, 0.0004, 0.0010, 0.0015, 0.0019, 0.0021, 0.0023, 0.0024, 0.0026,\n",
            "        0.0027, 0.0029, 0.0031, 0.0034, 0.0037, 0.0041, 0.0045, 0.0050, 0.0055,\n",
            "        0.0061, 0.0068, 0.0075, 0.0083, 0.0091, 0.0100, 0.0110, 0.0120],\n",
            "       device='cuda:0', grad_fn=<MeanBackward1>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zBrzZOQILd4p"
      },
      "source": [
        ""
      ],
      "execution_count": 60,
      "outputs": []
    }
  ]
}